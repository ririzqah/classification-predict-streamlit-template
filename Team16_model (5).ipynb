{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Team16_model (5).ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8cvume7UQed",
        "outputId": "bc51ab5c-28d0-4b65-bedf-41d705363ea7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Libraries for data loading, data manipulation and data visulisation\n",
        "import nltk\n",
        "import re\n",
        "import csv\n",
        "import string\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns \n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Downloads\n",
        "#nlp = spacy.load('en')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Libraries for data preparation and model building\n",
        "# Preprocessing\n",
        "from collections import Counter\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
        "from nltk import SnowballStemmer, PorterStemmer, LancasterStemmer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import stopwords, wordnet  \n",
        "from sklearn.feature_extraction.text import CountVectorizer   \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.utils import resample\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "\n",
        "# Setting global constants to ensure notebook results are reproducible\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "sns.set(rc={'figure.figsize':(12,8)})"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKexTVVtUQeh",
        "outputId": "ba7b16df-0719-4682-8f0c-142b147b0e71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "source": [
        "df_train = pd.read_csv('train.csv')\n",
        "df_test = pd.read_csv('test_with_no_labels.csv')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-889db98ea874>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_with_no_labels.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh57bRrPUQei"
      },
      "source": [
        "pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
        "subs_url = r'url-web'\n",
        "df['message'] = df['message'].replace(to_replace = pattern_url, value = subs_url, regex = True)\n",
        "\n",
        "\n",
        "###test\n",
        "test['message'] = test['message'].replace(to_replace = pattern_url, value = subs_url, regex = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn2wFrL2UQei",
        "outputId": "cd33aa5a-1f88-4426-9f57-0f723b76e1ef"
      },
      "source": [
        "print(string.punctuation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCcDE0VtUQej"
      },
      "source": [
        "def remove_punctuation(post):\n",
        "    return ''.join([l for l in post if l not in string.punctuation])\n",
        "\n",
        "df['message'] = df['message'].apply(remove_punctuation)\n",
        "\n",
        "\n",
        "\n",
        "###test\n",
        "test['message'] = test['message'].apply(remove_punctuation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyJayJouUQek"
      },
      "source": [
        "# Remove all words below 3 characters\n",
        "df['newmessage'] = df['message'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
        "test['newmessage'] =test['message'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Pel4T6mUQek",
        "outputId": "d8356a90-d958-43b5-e8e9-8b0031dee3a7"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>message</th>\n",
              "      <th>tweetid</th>\n",
              "      <th>newmessage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>PolySciMajor EPA chief doesnt think carbon dio...</td>\n",
              "      <td>625221</td>\n",
              "      <td>PolySciMajor chief doesnt think carbon dioxide...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Its not like we lack evidence of anthropogenic...</td>\n",
              "      <td>126103</td>\n",
              "      <td>like lack evidence anthropogenic global warming</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>RT RawStory Researchers say we have three year...</td>\n",
              "      <td>698562</td>\n",
              "      <td>RawStory Researchers have three years climate ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>TodayinMaker WIRED  2016 was a pivotal year in...</td>\n",
              "      <td>573736</td>\n",
              "      <td>TodayinMaker WIRED 2016 pivotal year climate c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>RT SoyNovioDeTodas Its 2016 and a racist sexis...</td>\n",
              "      <td>466954</td>\n",
              "      <td>SoyNovioDeTodas 2016 racist sexist climate cha...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment                                            message  tweetid  \\\n",
              "0          1  PolySciMajor EPA chief doesnt think carbon dio...   625221   \n",
              "1          1  Its not like we lack evidence of anthropogenic...   126103   \n",
              "2          2  RT RawStory Researchers say we have three year...   698562   \n",
              "3          1  TodayinMaker WIRED  2016 was a pivotal year in...   573736   \n",
              "4          1  RT SoyNovioDeTodas Its 2016 and a racist sexis...   466954   \n",
              "\n",
              "                                          newmessage  \n",
              "0  PolySciMajor chief doesnt think carbon dioxide...  \n",
              "1    like lack evidence anthropogenic global warming  \n",
              "2  RawStory Researchers have three years climate ...  \n",
              "3  TodayinMaker WIRED 2016 pivotal year climate c...  \n",
              "4  SoyNovioDeTodas 2016 racist sexist climate cha...  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQea5pJ3UQel"
      },
      "source": [
        "df['newmessage'] = df['newmessage'].str.lower()\n",
        "test['newmessage'] = test['newmessage'].str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcHtshjwUQel",
        "outputId": "35de3ae4-d68a-42ec-83a8-e23c1c107c71"
      },
      "source": [
        "tokeniser = TreebankWordTokenizer()\n",
        "df['tokenised_message'] = df['newmessage'].apply(tokeniser.tokenize)\n",
        "df['tokenised_message'].iloc[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['rawstory',\n",
              " 'researchers',\n",
              " 'have',\n",
              " 'three',\n",
              " 'years',\n",
              " 'climate',\n",
              " 'change',\n",
              " 'before',\n",
              " 'it’s',\n",
              " 'late',\n",
              " 'urlweb',\n",
              " 'urlweb…']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dV7FZZQbUQem",
        "outputId": "e0dcf116-4546-4837-cc69-6a2a56d45ea4"
      },
      "source": [
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "def mbti_stemmer(words, stemmer):\n",
        "    return [stemmer.stem(word) for word in words]\n",
        "\n",
        "df['stemmed_message'] = df['tokenised_message'].apply(mbti_stemmer, args=(stemmer, ))\n",
        "\n",
        "for i, t in enumerate(df.iloc[15]['tokenised_message']):    \n",
        "    print ('{:20s} --> {:10s}'.format(t, df.iloc[15]['stemmed_message'][i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "glblctzn             --> glblctzn  \n",
            "dont                 --> dont      \n",
            "wan                  --> wan       \n",
            "na                   --> na        \n",
            "live                 --> live      \n",
            "forever              --> forev     \n",
            "nothing              --> noth      \n",
            "will                 --> will      \n",
            "because              --> becaus    \n",
            "climate              --> climat    \n",
            "change               --> chang     \n",
            "����️��              --> ����️��   \n",
            "taylorswift13        --> taylorswift13\n",
            "zaynmalik            --> zaynmalik \n",
            "urlweb               --> urlweb    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KnKyshRUQen",
        "outputId": "d6aa7c65-ad38-4136-8d5d-411907c4313f"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def mbti_lemma(words, lemmatizer):\n",
        "    return [lemmatizer.lemmatize(word) for word in words]  \n",
        "\n",
        "df['lemma_message'] = df['tokenised_message'].apply(mbti_lemma, args=(lemmatizer, ))  \n",
        "\n",
        "for i, t in enumerate(df.iloc[1]['tokenised_message']):    \n",
        "    print ('{:20s} --> {:10s}'.format(t, df.iloc[1]['lemma_message'][i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "like                 --> like      \n",
            "lack                 --> lack      \n",
            "evidence             --> evidence  \n",
            "anthropogenic        --> anthropogenic\n",
            "global               --> global    \n",
            "warming              --> warming   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAXu7WSTUQen"
      },
      "source": [
        "TFID = TfidfVectorizer(stop_words='english', \n",
        "                             min_df=2, \n",
        "                             max_df=0.90, \n",
        "                             ngram_range=(1, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UYqReIbUQeo"
      },
      "source": [
        "X = df['newmessage']\n",
        "y = df['sentiment']\n",
        "\n",
        "###test\n",
        "X_real = test['newmessage']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0egwabuUQeo"
      },
      "source": [
        "X_vec_t = TFID.fit_transform(X)\n",
        "\n",
        "###real\n",
        "X_vec_t_real = TFID.transform(X_real)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfKikN9cUQep"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_vec_t, y, test_size=0.20, random_state=32, stratify=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPD2FQFjUQep",
        "outputId": "95e522a4-ddca-4957-8b16-21886ff3deb4"
      },
      "source": [
        "### Linear SVC\n",
        "lsvc = LinearSVC()\n",
        "# Fit model to training data\n",
        "lsvc.fit(X_train, y_train)\n",
        "# Use trained model to run prediction on validation data\n",
        "lsvc_pred= lsvc.predict(X_test)\n",
        "print(\"Linear SVC Metrics\")\n",
        "print(metrics.classification_report(y_test, lsvc_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear SVC Metrics\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.76      0.51      0.61       259\n",
            "           0       0.56      0.41      0.47       471\n",
            "           1       0.76      0.86      0.81      1706\n",
            "           2       0.75      0.75      0.75       728\n",
            "\n",
            "    accuracy                           0.74      3164\n",
            "   macro avg       0.71      0.63      0.66      3164\n",
            "weighted avg       0.73      0.74      0.73      3164\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxWsSUMVUQep"
      },
      "source": [
        "lsvc_pred_real= lsvc.predict(X_vec_t_real)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rKgT7APUQep"
      },
      "source": [
        "test['sentiment'] = lsvc_pred_real"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9PY1M5FUQeq",
        "outputId": "36066199-693e-4ac2-8c7a-e0ee1aa7df32"
      },
      "source": [
        "test.head(25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>message</th>\n",
              "      <th>tweetid</th>\n",
              "      <th>newmessage</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Europe will now be looking to China to make su...</td>\n",
              "      <td>169760</td>\n",
              "      <td>europe will looking china make sure that alone...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Combine this with the polling of staffers re c...</td>\n",
              "      <td>35326</td>\n",
              "      <td>combine this with polling staffers climate cha...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The scary unimpeachable evidence that climate ...</td>\n",
              "      <td>224985</td>\n",
              "      <td>scary unimpeachable evidence that climate chan...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Karoli morgfair OsborneInk dailykos \\nPutin go...</td>\n",
              "      <td>476263</td>\n",
              "      <td>karoli morgfair osborneink dailykos putin jill...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RT FakeWillMoore Female orgasms cause global w...</td>\n",
              "      <td>872928</td>\n",
              "      <td>fakewillmoore female orgasms cause global warm...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>RT nycjim Trump muzzles employees of several g...</td>\n",
              "      <td>75639</td>\n",
              "      <td>nycjim trump muzzles employees several gov’t a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>bmastenbrook yes wrote that in 3rd yr Comp Sci...</td>\n",
              "      <td>211536</td>\n",
              "      <td>bmastenbrook wrote that comp ethics part told ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>RT climatehawk1 Indonesian farmers weather cli...</td>\n",
              "      <td>569434</td>\n",
              "      <td>climatehawk1 indonesian farmers weather climat...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>RT guardian British scientists face a ‘huge hi...</td>\n",
              "      <td>315368</td>\n",
              "      <td>guardian british scientists face ‘huge hit’ cu...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Aid For Agriculture  Sustainable agriculture a...</td>\n",
              "      <td>591733</td>\n",
              "      <td>agriculture sustainable agriculture climate ch...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>There is no climate change Globalists urlweb</td>\n",
              "      <td>91983</td>\n",
              "      <td>there climate change globalists urlweb</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Biggest threat to our economy is climate chang...</td>\n",
              "      <td>67249</td>\n",
              "      <td>biggest threat economy climate change urlweb</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>RT 100isNow Hes CEO of a company that lied abo...</td>\n",
              "      <td>143459</td>\n",
              "      <td>100isnow company that lied about climate chang...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>RT VICE Venice could be swallowed by water wit...</td>\n",
              "      <td>663535</td>\n",
              "      <td>vice venice could swallowed water within centu...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>RT TotalCardsMove Its so warm outside because ...</td>\n",
              "      <td>20476</td>\n",
              "      <td>totalcardsmove warm outside because climate ch...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Niggas ask me what my inspiration was I told e...</td>\n",
              "      <td>815297</td>\n",
              "      <td>niggas what inspiration told global warming fe...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>RT SenSanders We have a presidentelect who doe...</td>\n",
              "      <td>274098</td>\n",
              "      <td>sensanders have presidentelect doesnt believe ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>RT sccscot We welcome recommendations publishe...</td>\n",
              "      <td>30045</td>\n",
              "      <td>sccscot welcome recommendations published msps...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>RT yajairaxlove MidNovember amp its hot as hel...</td>\n",
              "      <td>681487</td>\n",
              "      <td>yajairaxlove midnovember hell global warming hoax</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Recordbreaking climate change pushes world int...</td>\n",
              "      <td>708966</td>\n",
              "      <td>recordbreaking climate change pushes world int...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>RT AdamsFlaFan BigOilOwned House science chair...</td>\n",
              "      <td>393689</td>\n",
              "      <td>adamsflafan bigoilowned house science chairman...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>RT thehill Bill Nye slams CNN for having clima...</td>\n",
              "      <td>186705</td>\n",
              "      <td>thehill bill slams having climate change skept...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Michael Moore calls Trump’s actions on climate...</td>\n",
              "      <td>233977</td>\n",
              "      <td>michael moore calls trump’s actions climate ch...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>RT SenSanders We have a presidentelect who doe...</td>\n",
              "      <td>525794</td>\n",
              "      <td>sensanders have presidentelect doesnt believe ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>LeoDiCaprio s BeforeTheFlood is such a masterp...</td>\n",
              "      <td>863649</td>\n",
              "      <td>leodicaprio beforetheflood such masterpiece ne...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              message  tweetid  \\\n",
              "0   Europe will now be looking to China to make su...   169760   \n",
              "1   Combine this with the polling of staffers re c...    35326   \n",
              "2   The scary unimpeachable evidence that climate ...   224985   \n",
              "3   Karoli morgfair OsborneInk dailykos \\nPutin go...   476263   \n",
              "4   RT FakeWillMoore Female orgasms cause global w...   872928   \n",
              "5   RT nycjim Trump muzzles employees of several g...    75639   \n",
              "6   bmastenbrook yes wrote that in 3rd yr Comp Sci...   211536   \n",
              "7   RT climatehawk1 Indonesian farmers weather cli...   569434   \n",
              "8   RT guardian British scientists face a ‘huge hi...   315368   \n",
              "9   Aid For Agriculture  Sustainable agriculture a...   591733   \n",
              "10       There is no climate change Globalists urlweb    91983   \n",
              "11  Biggest threat to our economy is climate chang...    67249   \n",
              "12  RT 100isNow Hes CEO of a company that lied abo...   143459   \n",
              "13  RT VICE Venice could be swallowed by water wit...   663535   \n",
              "14  RT TotalCardsMove Its so warm outside because ...    20476   \n",
              "15  Niggas ask me what my inspiration was I told e...   815297   \n",
              "16  RT SenSanders We have a presidentelect who doe...   274098   \n",
              "17  RT sccscot We welcome recommendations publishe...    30045   \n",
              "18  RT yajairaxlove MidNovember amp its hot as hel...   681487   \n",
              "19  Recordbreaking climate change pushes world int...   708966   \n",
              "20  RT AdamsFlaFan BigOilOwned House science chair...   393689   \n",
              "21  RT thehill Bill Nye slams CNN for having clima...   186705   \n",
              "22  Michael Moore calls Trump’s actions on climate...   233977   \n",
              "23  RT SenSanders We have a presidentelect who doe...   525794   \n",
              "24  LeoDiCaprio s BeforeTheFlood is such a masterp...   863649   \n",
              "\n",
              "                                           newmessage  sentiment  \n",
              "0   europe will looking china make sure that alone...          1  \n",
              "1   combine this with polling staffers climate cha...          1  \n",
              "2   scary unimpeachable evidence that climate chan...          1  \n",
              "3   karoli morgfair osborneink dailykos putin jill...          1  \n",
              "4   fakewillmoore female orgasms cause global warm...          0  \n",
              "5   nycjim trump muzzles employees several gov’t a...          1  \n",
              "6   bmastenbrook wrote that comp ethics part told ...          1  \n",
              "7   climatehawk1 indonesian farmers weather climat...          1  \n",
              "8   guardian british scientists face ‘huge hit’ cu...          2  \n",
              "9   agriculture sustainable agriculture climate ch...          1  \n",
              "10             there climate change globalists urlweb         -1  \n",
              "11       biggest threat economy climate change urlweb          1  \n",
              "12  100isnow company that lied about climate chang...          2  \n",
              "13  vice venice could swallowed water within centu...          2  \n",
              "14  totalcardsmove warm outside because climate ch...          0  \n",
              "15  niggas what inspiration told global warming fe...          0  \n",
              "16  sensanders have presidentelect doesnt believe ...          1  \n",
              "17  sccscot welcome recommendations published msps...          2  \n",
              "18  yajairaxlove midnovember hell global warming hoax         -1  \n",
              "19  recordbreaking climate change pushes world int...          2  \n",
              "20  adamsflafan bigoilowned house science chairman...          2  \n",
              "21  thehill bill slams having climate change skept...          2  \n",
              "22  michael moore calls trump’s actions climate ch...          2  \n",
              "23  sensanders have presidentelect doesnt believe ...          1  \n",
              "24  leodicaprio beforetheflood such masterpiece ne...          1  "
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl1eVBmMUQeq"
      },
      "source": [
        "test[['tweetid','sentiment']].to_csv('Team16.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWjjo6gxUQeq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}